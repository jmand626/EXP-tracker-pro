{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgFdoBCHAJZGxLegpm+5K1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmand626/EXP-tracker-pro/blob/main/ExperimentTracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXFKmU5b4znP",
        "outputId": "fc2aa672-5f05-4150-f8e1-586b03a0c459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning https://github.com/jmand626/PyTorchMLEngine-Custom-Dataset-Project.git...\n",
            "Cloning into 'PyTorchMLEngine-Custom-Dataset-Project'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 24 (delta 6), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (24/24), 21.93 KiB | 5.48 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Current working directory: /content/PyTorchMLEngine-Custom-Dataset-Project\n",
            "Copying dataset from Google Drive to local VM...\n",
            "Train set copied.\n",
            "Test set copied.\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# 1. Imports and Setup\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clone Repo\n",
        "repo_url = \"https://github.com/jmand626/PyTorchMLEngine-Custom-Dataset-Project.git\"\n",
        "repo_name = \"PyTorchMLEngine-Custom-Dataset-Project\"\n",
        "\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Cloning {repo_url}...\")\n",
        "    !git clone {repo_url}\n",
        "\n",
        "# Add to sys.path\n",
        "os.chdir(repo_name)\n",
        "sys.path.append(os.getcwd())\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Install TensorBoard and Torchinfo\n",
        "!pip install -q torchinfo tensorboard\n",
        "\n",
        "# Define Paths\n",
        "gdrive_train_dir = \"/content/drive/MyDrive/data/fgvc-aircraft-2013b/train\"\n",
        "gdrive_test_dir = \"/content/drive/MyDrive/data/fgvc-aircraft-2013b/test\"\n",
        "local_train_dir = \"/content/train\"\n",
        "local_test_dir = \"/content/test\"\n",
        "\n",
        "# COPY DATA TO LOCAL VM (Crucial for speed)\n",
        "print(\"Copying dataset from Google Drive to local VM...\")\n",
        "if not os.path.exists(local_train_dir):\n",
        "    shutil.copytree(gdrive_train_dir, local_train_dir)\n",
        "    print(\"Train set copied.\")\n",
        "else:\n",
        "    print(\"Train set already exists locally.\")\n",
        "\n",
        "if not os.path.exists(local_test_dir):\n",
        "    shutil.copytree(gdrive_test_dir, local_test_dir)\n",
        "    print(\"Test set copied.\")\n",
        "else:\n",
        "    print(\"Test set already exists locally.\")\n",
        "\n",
        "# Setup Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import setup_dataholders\n",
        "import importlib\n",
        "importlib.reload(setup_dataholders) # Ensure we have the latest version\n",
        "\n",
        "# Define Transforms (Standard ImageNet Normalization)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create DataLoaders\n",
        "# Note: We are using workers=0 to avoid the deadlock issue you faced before\n",
        "train_dataloader, test_dataloader, class_names = setup_dataholders.create_dataloaders(\n",
        "    train_directory=local_train_dir,\n",
        "    test_directory=local_test_dir,\n",
        "    data_transforms=manual_transforms,\n",
        "    batch_size=32,\n",
        "    workers=0\n",
        ")\n",
        "\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "print(f\"Classes: {class_names[:10]}...\") # Print first 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DztmO99YCc_i",
        "outputId": "f1b93b52-7f2c-4e8e-dd2e-71dfdfeaa7d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 100\n",
            "Classes: ['707_320', '727_200', '737_200', '737_300', '737_400', '737_500', '737_600', '737_700', '737_800', '737_900']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "def create_writer(experiment_name: str,\n",
        "                  model_name: str,\n",
        "                  extra: str=None) -> SummaryWriter:\n",
        "    \"\"\"\n",
        "    Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
        "    \"\"\"\n",
        "    # Get timestamp of current date\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    if extra:\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
        "    else:\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
        "\n",
        "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
        "    return SummaryWriter(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "qGvpZ3RnCe48"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn as nn\n",
        "from model_backbone import train_step, run_test_step # Import steps from your file\n",
        "\n",
        "def train_with_tracking(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device,\n",
        "          writer: SummaryWriter) -> Dict[str, List]:\n",
        "\n",
        "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # 1. Train Step\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           dataloader=train_dataloader,\n",
        "                                           loss_fn=loss_fn,\n",
        "                                           optimizer=optimizer,\n",
        "                                           device=device)\n",
        "        # 2. Test Step\n",
        "        test_loss, test_acc = run_test_step(model=model,\n",
        "                                        dataloader=test_dataloader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        device=device)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1} | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f}\")\n",
        "\n",
        "        # 3. Log to TensorBoard\n",
        "        if writer:\n",
        "            writer.add_scalars(main_tag=\"Loss\",\n",
        "                               tag_scalar_dict={\"train_loss\": train_loss, \"test_loss\": test_loss},\n",
        "                               global_step=epoch)\n",
        "            writer.add_scalars(main_tag=\"Accuracy\",\n",
        "                               tag_scalar_dict={\"train_acc\": train_acc, \"test_acc\": test_acc},\n",
        "                               global_step=epoch)\n",
        "            # Close the writer\n",
        "            writer.close()\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "vddG8SoAItSN"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}